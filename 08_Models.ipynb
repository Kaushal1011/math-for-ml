{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fitted-selling",
   "metadata": {},
   "source": [
    "# Math for ML: Chapter 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "popular-detail",
   "metadata": {},
   "source": [
    "- Data as vectors\n",
    "- Models as functions\n",
    "- Models as probability distributions\n",
    "> Instead of considering a predictor as a single function, we could consider predictors to be probabilistic models, i.e., models describing the distribution of possible functions. \n",
    "- Learning is finding parameters "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "immune-scroll",
   "metadata": {},
   "source": [
    "### Empirical Risk Minimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "increased-disorder",
   "metadata": {},
   "source": [
    "You should know the answer to 4 main questions.\n",
    "\n",
    "__Section 8.2.1__ What is the set of functions we allow the predictor to take?\n",
    "\n",
    "> Use of affine functions or Linear functions [Affine Functions](https://mathworld.wolfram.com/AffineFunction.html)\n",
    "\n",
    "__Section 8.2.2__ How do we measure how well the predictor performs on\n",
    "the training data?\n",
    "\n",
    "> By using special class of functions called loss functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedicated-blade",
   "metadata": {},
   "source": [
    "- Data in ML is IID\n",
    "\n",
    "The word independent means that two data points (xi,yi) and (xj, yj) do not statistically depend on each other, meaning that the empirical mean is a good estimate of the population mean.\n",
    "> The empirical mean vector is the arithmetic average of the observations for each variable.\n",
    "\n",
    "existance of empirical risk.\n",
    "\n",
    "Many machine learning tasks are specified with an associated\n",
    "performance measure, e.g., accuracy of prediction or root mean squared\n",
    "error. The performance measure could be more complex, be cost sensitive,\n",
    "and capture details about the particular application. In principle, the design of the loss function for empirical risk minimization should correspond\n",
    "directly to the performance measure specified by the machine learning\n",
    "task. In practice, there is often a mismatch between the design of the loss\n",
    "function and the performance measure. This could be due to issues such\n",
    "as ease of implementation or efficiency of optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informed-marketplace",
   "metadata": {},
   "source": [
    "__Section 8.2.3__ How do we construct predictors from only training data\n",
    "that performs well on unseen test data?\n",
    "\n",
    "The aim of training a machine learning predictor is so that we can perform well on unseen data, i.e., the predictor generalizes well. We simulate this unseen data by holding out a proportion of the whole dataset. test set This hold out set is referred to as the test set. Given a sufficiently rich class of functions for the predictor f, we can essentially memorize the training data to obtain zero empirical risk. While this is great to minimize the loss (and therefore the risk) on the training data, we would not expect the predictor to generalize well to unseen data. In practice, we have only a finite set of data, and hence we split our data into a training and a test set. \n",
    "\n",
    "we need to somehow bias the search for the minimizer of empirical risk by introducing a penalty term, which makes it harder for the optimizer to return an overly flexible predictor. In machine learning, regularization the penalty term is referred to as regularization. Regularization is a way to compromise between accurate solution of empirical risk minimization and the size or complexity of the solution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "failing-producer",
   "metadata": {},
   "source": [
    "### Parameter Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "backed-collapse",
   "metadata": {},
   "source": [
    "we will see how to use probability distributions to model our uncertainty due to the observation process and our uncertainty in the parameters of our predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accredited-singer",
   "metadata": {},
   "source": [
    "#### Maximum Likelihood Estimation\n",
    "\n",
    "- [Super Quick Insight](https://stats.stackexchange.com/questions/299169/what-is-the-meaning-of-pyx-theta-and-px-theta-and-px-yz)\n",
    "\n",
    "- [Towards DS Article](https://towardsdatascience.com/probability-concepts-explained-maximum-likelihood-estimation-c7b4342fdbb1)\n",
    "\n",
    "- Best Source is textbook 8.3.1 Section. Nice explaination\n",
    "\n",
    "Ye karke last mai normal equation hi laya tha textbook wala next chapter mai :O\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "breathing-tourist",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "drawn-anger",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "first-trader",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "second-bench",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "allied-repository",
   "metadata": {},
   "source": [
    "__Section 8.2.4__ What is the procedure for searching over the space of models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "geographic-highway",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protective-situation",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
