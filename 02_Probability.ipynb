{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability and Infromation Theory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___Scraped from MIT deep learning book___ by Ian Goodfellow, Yoshua Bengio & Aaron Courville"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probability theory is a mathematical framework for representing uncertain\n",
    "statements. It provides a means of quantifying uncertainty and axioms for deriving\n",
    "new uncertain statements. In artificial intelligence applications, we use probability\n",
    "theory in two major ways. First, the laws of probability tell us how AI systems\n",
    "should reason, so we design our algorithms to compute or approximate various\n",
    "expressions derived using probability theory. Second, we can use probability and\n",
    "statistics to theoretically analyze the behavior of proposed AI systems. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many branches of computer science deal mostly with entities that are entirely\n",
    "deterministic and certain. A programmer can usually safely assume that a CPU will\n",
    "execute each machine instruction flawlessly. Errors in hardware do occur, but are\n",
    "rare enough that most software applications do not need to be designed to account\n",
    "for them. Given that many computer scientists and software engineers work in a\n",
    "relatively clean and certain environment, it can be surprising that machine learning\n",
    "makes heavy use of probability theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three possible sources of uncertainty:\n",
    "1. Inherent stochasticity in the system being modeled. For example, most interpretations of quantum mechanics describe the dynamics of subatomic particles as being probabilistic. We can also create theoretical scenarios that we postulate to have random dynamics, such as a hypothetical card game where we assume that the cards are truly shuffled into a random order\n",
    "2. Incomplete observability. Even deterministic systems can appear stochastic when we cannot observe all of the variables that drive the behavior of the system. For example, in the Monty Hall problem, a game show contestant is asked to choose between three doors and wins a prize held behind the chosen door. Two doors lead to a goat while a third leads to a car. The outcome given the contestant’s choice is deterministic, but from the contestant’s point of view, the outcome is uncertain.\n",
    "3. Incomplete modeling. When we use a model that must discard some of the information we have observed, the discarded information results in uncertainty in the model’s predictions. For example, suppose we build a robot that can exactly observe the location of every object around it. If the robot discretizes space when predicting the future location of these objects, then the discretization makes the robot immediately become uncertain about the precise position of objects: each object could be anywhere within the discrete cell that it was observed to occupy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`\"In many cases, it is more practical to use a simple but uncertain rule rather\n",
    "than a complex but certain one, even if the true rule is deterministic and our\n",
    "modeling system has the fidelity to accommodate a complex rule. For example, the\n",
    "simple rule “Most birds fly” is cheap to develop and is broadly useful, while a rule\n",
    "of the form, “Birds fly, except for very young birds that have not yet learned to\n",
    "fly, sick or injured birds that have lost the ability to fly, flightless species of birds\n",
    "including the cassowary, ostrich and kiwi. . .” is expensive to develop, maintain and\n",
    "communicate, and after all of this effort is still very brittle and prone to failure.\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`degree of belief`:we use probability to represent a\n",
    "degree of belief, with 1 indicating absolute certainty that the patient has the flu\n",
    "and 0 indicating absolute certainty that the patient does not have the flu.(in a doctor diagnosis example)\n",
    "`frequentist probability`:kind of probability, related directly to the rates at which events occur\n",
    "`Bayesian probability`: probability related to qualitative levels of uncertainity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probability can be seen as the extension of logic to deal with uncertainty. Logic\n",
    "provides a set of formal rules for determining what propositions are implied to\n",
    "be true or false given the assumption that some other set of propositions is true\n",
    "or false. Probability theory provides a set of formal rules for determining the\n",
    "likelihood of a proposition being true given the likelihood of other propositions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probability can be seen as the extension of logic to deal with uncertainty. Logic\n",
    "provides a set of formal rules for determining what propositions are implied to\n",
    "be true or false given the assumption that some other set of propositions is true\n",
    "or false. Probability theory provides a set of formal rules for determining the\n",
    "likelihood of a proposition being true given the likelihood of other propositions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Variables\n",
    "\n",
    "A random variable is a variable (function is a better representation maybe?) that can take on different values randomly. On\n",
    "its own, a random variable is just a description of the states that are possible; it\n",
    "must be coupled with a probability distribution that specifies how likely each of\n",
    "these states are.\n",
    "Random variables may be discrete or continuous. A discrete random variable\n",
    "is one that has a finite or countably infinite number of states.  A continuous random variable is\n",
    "associated with a real value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability Distributions\n",
    "\n",
    "A `probability distribution` is a description of how likely a random variable or\n",
    "set of random variables is to take on each of its possible states. The way we\n",
    "describe probability distributions depends on whether the variables are discrete or\n",
    "continuous.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discrete Variables and Probability Mass Functions\n",
    "\n",
    "A probability distribution over discrete variables may be described using a ___probability mass function (PMF)___."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability mass function maps from a state of a random variable to\n",
    "the probability of that random variable taking on that state. The probability\n",
    "that x = x is denoted as P(x), with a probability of 1 indicating that x = x is\n",
    "certain and a probability of 0 indicating that x = x is impossible.\n",
    "\n",
    "Probability mass functions can act on many variables at the same time. Such\n",
    "a probability distribution over many variables is known as a ___joint probability\n",
    "distribution___\n",
    "\n",
    "To be a probability mass function on a random variable x, a function P must\n",
    "satisfy the following properties:\n",
    "- The domain of P must be the set of all possible states of x.\n",
    "- ∀x ∈ x,0 ≤ P(x) ≤ 1. An impossible event has probability 0 and no state can be less probable than that. Likewise, an event that is guaranteed to happen has probability 1, and no state can have a greater chance of occurring.\n",
    "- $\\sum_{x∈x} P(x) = 1.$ We refer to this property as being normalized. Without this property, we could obtain probabilities greater than one by computing the probability of one of many events occurring."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous Variables and Probability Density Functions\n",
    "\n",
    "\n",
    "When working with continuous random variables, we describe probability distributions using a __probability density function (PDF)___ rather than a probability\n",
    "mass function.\n",
    "\n",
    "To be a probability density function, a function p must satisfy the\n",
    "following properties:\n",
    "- The domain of p must be the set of all possible states of x. \n",
    "- ∀x ∈ x, p(x) ≥ 0. Note that we do not require p(x) ≤ 1.\n",
    "- $\\int$ p(x)dx = 1\n",
    "\n",
    "A probability density function p(x) does not give the probability of a specific\n",
    "state directly, instead the probability of landing inside an infinitesimal region with\n",
    "volume δx is given by p(x)δx. We can integrate the density function to find the actual probability mass of a\n",
    "set of points. Specifically, the probability that x lies in some set S is given by the\n",
    "integral of p(x) over that set.\n",
    "\n",
    "![](img/pdfpmf.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Marginal Probability\n",
    "\n",
    "Sometimes we know the probability distribution over a set of variables and we want\n",
    "to know the probability distribution over just a subset of them. The probability\n",
    "distribution over the subset is known as the marginal probability distribution.\n",
    "\n",
    "For example, suppose we have discrete random variables x and y, and we know\n",
    "P(x, y). We can find P(x) with the sum rule:\n",
    "\n",
    "∀x ∈ x,$P$(x = $x$) = $\\sum_y$\n",
    "$P$(x = $x$, y = $y$).\n",
    "\n",
    "For continuous variables, we need to use integration instead of summation:\n",
    "$p(x)$ =$\\int$\n",
    "$p$($x, y$)$dy$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional Probability\n",
    "\n",
    "![](img/condition.PNG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Chain Rule of Conditional Probabilities\n",
    "\n",
    "Any joint probability distribution over many random variables may be decomposed\n",
    "into conditional distributions over only one variable:\n",
    "![](img/chainprob.PNG)\n",
    "\n",
    "This observation is known as the chain rule or product rule of probability. It follows immediately from the definition of conditional probability in equation 3.5\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Independence and Conditional Independence\n",
    "\n",
    "![](img/indep.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expectation, Variance and Covariance\n",
    "\n",
    "The expectation or expected value of some function f(x) with respect to a\n",
    "probability distribution P(x) is the average or mean value that f takes on when x\n",
    "is drawn from P.\n",
    "\n",
    "![](img/expect.PNG)\n",
    "\n",
    "![](img/varcovar.PNG)\n",
    "\n",
    "measures such as correlation normalize the\n",
    "contribution of each variable in order to measure only how much the variables are\n",
    "related, rather than also being affected by the scale of the separate variables.\n",
    "\n",
    "![](img/covar.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Common Probability Distributions`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bernoulli Distribution\n",
    "\n",
    "The Bernoulli distribution is a distribution over a single binary random variable.\n",
    "It is controlled by a single parameter φ ∈ [0, 1], which gives the probability of the\n",
    "random variable being equal to 1.\n",
    "\n",
    "![](img/berndist.PNG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinoulli Distribution\n",
    "\n",
    "The multinoulli or categorical distribution is a distribution over a single discrete\n",
    "variable with k different states, where k is finite.\n",
    "\n",
    "The multinoulli distribution is parametrized by a vector $p ∈ [0, 1]^{k−1}$\n",
    ", where pi gives the probability of the i-th\n",
    "state. The final, k-th state’s probability is given by $1 − 1^Tp$. Note that we must\n",
    "constrain $1^Tp ≤ 1$. Multinoulli distributions are often used to refer to distributions\n",
    "over categories of objects, so we do not usually assume that state 1 has numerical\n",
    "value 1, etc. For this reason, we do not usually need to compute the expectation\n",
    "or variance of multinoulli-distributed random variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Gaussian Distribution`\n",
    "\n",
    "The most commonly used distribution over real numbers is the normal distribution, also known as the Gaussian distribution:\n",
    "![](img/gauss.PNG)\n",
    "![](img/gauss2.PNG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The normal distribution is a good default choice for two major reasons:\n",
    "\n",
    "- First, many distributions we wish to model are truly close to being normal distributions. The central limit theorem shows that the sum of many independent random variables is approximately normally distributed. This means that in practice, many complicated systems can be modeled successfully as normally distributed noise, even if the system can be decomposed into parts with more structured behavior\n",
    "- Second, out of all possible probability distributions with the same variance, the normal distribution encodes the maximum amount of uncertainty over the real numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/gauss3.PNG)\n",
    "![](img/gauss4.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exponential and Laplace Distributions\n",
    "\n",
    "![](img/laplace.PNG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
