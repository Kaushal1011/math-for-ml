{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Algebra\n",
    "\n",
    "___Scraped from MIT deep learning book___ by Ian Goodfellow, Yoshua Bengio & Aaron Courville\n",
    "\n",
    "## Reference material\n",
    "[MIT 18.06 - W. Gilbert Strang](https://www.youtube.com/playlist?list=PLE7DDD91010BC51F8)\n",
    "\n",
    "\n",
    "## Scalars, Vectors, Matrices and Tensors\n",
    "\n",
    "- __Scalars__: A scalar is just a single number.\n",
    "- __Vectors__: A vector is an array of numbers. The numbers are arranged in order. We can identify each individual number by its index in that ordering.\n",
    "- __Matrices__: A matrix is a 2-D array of numbers, so each element is identified by two indices instead of just one. \n",
    "- __Tensors__: In some cases we will need an array with more than two axes. In the general case, an array of numbers arranged on a regular grid with a variable number of axes is known as a tensor.\n",
    "\n",
    "One important operation on matrices is transpose.\n",
    "\n",
    "\n",
    "\n",
    "## Multiplying Matrices and Vectors\n",
    "\n",
    "- Matrix Product : C = AB\n",
    "- element-wise product or Hadamard product\n",
    "- dot product\n",
    "\n",
    "__read about properties of matrices__\n",
    "\n",
    "## Identity and Inverse Matrices\n",
    "\n",
    "$ A^{-1}.A=I$\n",
    "\n",
    "- $A^{-1}$ is the inverse of A.\n",
    "- I is identity matrix.\n",
    "\n",
    "## Linear Dependance and Span\n",
    "\n",
    "- chapter 3 and chapter 4 of Introduction Linear Algebra by Gilbert Strang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Norms\n",
    "\n",
    "Sometimes we need to measure the size of a vector. In machine learning, we usually\n",
    "measure the size of vectors using a function called a norm. Formally, the L\n",
    "p norm\n",
    "is given by\n",
    "\n",
    "$ ||x||_p = \\left(\\sum_i{|x_i|^p}\\right)^{\\frac{1}{p}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Norms, including the L\n",
    "p norm, are functions mapping vectors to non-negative\n",
    "values. On an intuitive level, the norm of a vector x measures the distance from\n",
    "the origin to the point x. More rigorously, a norm is any function f that satisfies\n",
    "the following properties:\n",
    "- f (x) = 0 ⇒ x = 0\n",
    "- f (x + y) ≤ f(x) + f (y) (the triangle inequality)\n",
    "- ∀α ∈ R, f (αx) = |α|f (x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The L\n",
    "2 norm, with p = 2, is known as the Euclidean norm. It is simply the\n",
    "Euclidean distance from the origin to the point identified by x. The L\n",
    "2 norm is\n",
    "used so frequently in machine learning that it is often denoted simply as ||x||, with\n",
    "the subscript 2 omitted. It is also common to measure the size of a vector using\n",
    "the squared L\n",
    "2 norm, which can be calculated simply as $x^Tx$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The L\n",
    "1 norm is commonly used in machine learning when the difference between\n",
    "zero and nonzero elements is very important. Every time an element of x moves\n",
    "away from 0 by $\\epsilon$, the L\n",
    "1 norm increases by $\\epsilon$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some other norms are:\n",
    "- max norm\n",
    "- Frobenius norm : For measuring the size of a matrix (analogous to L2 norm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Special Kinds of Matrices and Vectors\n",
    "\n",
    "- Diagonal Matrices : Self Explanatory\n",
    "- Symmetric matix : $A=A^T$\n",
    "- Unit vector\n",
    "- Orthogonal Matrix : $ A^T.A=A.A^T=I$\n",
    "- orthonormal vectors : $x^T.y=0$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eigendecomposition\n",
    "\n",
    "Much as we can discover something about the true nature of an integer by\n",
    "decomposing it into prime factors, we can also decompose matrices in ways that\n",
    "show us information about their functional properties that is not obvious from the\n",
    "representation of the matrix as an array of elements.\n",
    "\n",
    "One of the most widely used kinds of matrix decomposition is called eigendecomposition, in which we decompose a matrix into a set of eigenvectors and\n",
    "eigenvalues.\n",
    "\n",
    "\n",
    "An eigenvector of a square matrix A is a non-zero vector v such that multiplication by A alters only the scale of v:\n",
    "Av = λv.\n",
    "\n",
    "The scalar λ is known as the eigenvalue corresponding to this eigenvector\n",
    "\n",
    "The eigendecomposition of A is then given by\n",
    "A = $V diag(λ)V^{−1}$.\n",
    "where V is the Eigenvector Matrix\n",
    "\n",
    "Not every matrix can be decomposed into eigenvalues and eigenvectors. In some cases, the decomposition exists, but may involve complex rather than real numbers. Fortunately, in this book, we usually need to decompose only a specific class of\n",
    "matrices that have a simple decomposition. Specifically, every real symmetric\n",
    "matrix can be decomposed into an expression using only real-valued eigenvectors\n",
    "and eigenvalues:\n",
    "\n",
    "$ A=QΛQ^T $\n",
    "where Q is the orthogonal matrix composed of eigenvectors of A and Λ is a diagonal matrix.\n",
    "\n",
    "While any real symmetric matrix A is guaranteed to have an eigendecomposition, the eigendecomposition may not be unique. If any two or more eigenvectors\n",
    "share the same eigenvalue, then any set of orthogonal vectors lying in their span\n",
    "are also eigenvectors with that eigenvalue, and we could equivalently choose a Q\n",
    "using those eigenvectors instead. By convention, we usually sort the entries of Λ\n",
    "in descending order. Under this convention, the eigendecomposition is unique only\n",
    "if all of the eigenvalues are unique.\n",
    "\n",
    "A matrix whose eigenvalues are all positive is called positive definite. A\n",
    "matrix whose eigenvalues are all positive or zero-valued is called positive semidefinite. Likewise, if all eigenvalues are negative, the matrix is negative definite, and\n",
    "if all eigenvalues are negative or zero-valued, it is negative semidefinite. Positive\n",
    "semidefinite matrices are interesting because they guarantee that $∀x, x^TAx ≥ 0$. Positive definite matrices additionally guarantee that $x^TAx = 0 ⇒ x = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD : Singular Value Decomposition\n",
    "\n",
    "The singular value decomposition (SVD) provides another way to factorize\n",
    "a matrix, into singular vectors and singular values. The SVD allows us to\n",
    "discover some of the same kind of information as the eigendecomposition.\n",
    "\n",
    "Every real matrix has a singular value\n",
    "decomposition, but the same is not true of the eigenvalue decomposition.\n",
    "\n",
    "![](img/la.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Moore-Penrose Pseudoinverse\n",
    "\n",
    "If A is taller than it is wide, then it is possible for Ax=b equation to have\n",
    "no solution. If A is wider than it is tall, then there could be multiple possible\n",
    "solutions.\n",
    "\n",
    "![](img/la2.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Trace Operator\n",
    "\n",
    "The trace operator gives the sum of all of the diagonal entries of a matrix:\n",
    "\n",
    "$Tr(A) =\\sum_iAi,i$.\n",
    "\n",
    "\n",
    "The trace operator is useful for a variety of reasons. Some operations that are\n",
    "difficult to specify without resorting to summation notation can be specified using matrix products and the trace operator. For example, the trace operator provides\n",
    "an alternative way of writing the Frobenius norm of a matrix:\n",
    "\n",
    "$||A||F = \\sqrt{Tr(AA^T)}.$\n",
    "\n",
    "__Read Propersties f Trace operator__\n",
    "\n",
    "## The Determinant\n",
    "\n",
    "The determinant of a square matrix, denoted det(A), is a function mapping\n",
    "matrices to real scalars. The determinant is equal to the product of all the\n",
    "eigenvalues of the matrix.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example PCA : Principal Component Analysis\n",
    "\n",
    "One simple machine learning algorithm, principal components analysis or PCA\n",
    "can be derived using only knowledge of basic linear algebra.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
